{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e421794d",
      "metadata": {
        "id": "e421794d"
      },
      "source": [
        "## Model Improvements/ Data processing\n",
        "\n",
        "To improve the performance of the various models that will be implemented, there are certain processing steps that can be done with the data to help improve the accuracy of the classifier\n",
        "\n",
        "1. Removing any punctuation and normalising all capitilisations in each of the documents text. This is because punctuation and the capitilisations shouldn't have an affect on what the abstract is talking about, so therefore should not be considered in our classifier. This was done by taking the column in the dataframe and removing all string.punctuation and converting all to str.lower.\n",
        "\n",
        "2. We can remove common words that occur in English that aren't specific to the abstract and so therefore we believe won't play a major role in determining the class. These are words such as \"and\", \"of\". These can be removed by specifying the argument stop_words='english' when creating using CountVectorizer()\n",
        "\n",
        "3. We can set a maximum frequency threshold for words to be considered in the classifier. similar to the removal of common english words, they may not be useful in determining certain classes as they appear too within all classes. This was done by specifying the argument max_df=0.9 when creating using CountVectorizer(). So by setting max_df = 0.9, any word that appears in more than 90% of the documents are discarded. On the flip side We can set the minimum frequency threshold for words we want to include in our model.\n",
        "\n",
        "4. In our data, we can specify that we want phrases of words to be included as well. This is specifying the ngram range. For this data, the ngram_range 1-3 words concatonated together, so this will consider not only the indivdiual words, but pair of words and triplets of words. This was done by specifying the argument ngram_range=(1,3) when creating using CountVectorizer(). This is because certain words may not have much meaning by itself in a sentence, but when joined with the words next to it, it can have a significant change in meaning. Some examples include homo-sapiens where homo or sapiens may not have signfinciant meaning alone but together have a very specific meaning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08a9cfd6",
      "metadata": {
        "id": "08a9cfd6"
      },
      "source": [
        "# Import libraries/ Training and Test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc8d49f",
      "metadata": {
        "id": "abc8d49f"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import pandas as pd\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12017913",
      "metadata": {
        "id": "12017913"
      },
      "outputs": [],
      "source": [
        "# importing training and test datasets\n",
        "trg = pd.read_csv(\"/Users/pangnakh/Downloads/trg.csv\")\n",
        "tst = pd.read_csv(\"/Users/pangnakh/Desktop/tst.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a83e688",
      "metadata": {
        "id": "0a83e688"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41ac280",
      "metadata": {
        "id": "b41ac280"
      },
      "outputs": [],
      "source": [
        "# Make sure all punctuation is removed from documents\n",
        "trg['abstract'] = trg['abstract'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "trg.to_csv('punctuation.csv',index= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e45a280",
      "metadata": {
        "id": "8e45a280"
      },
      "outputs": [],
      "source": [
        "# Make sure all documents converted to lowercase\n",
        "trg['abstract'] = trg['abstract'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e00ebb",
      "metadata": {
        "id": "44e00ebb"
      },
      "outputs": [],
      "source": [
        "# Calculate and produce a table of each of the class prior probabilities\n",
        "class_counts = Y_train.value_counts()\n",
        "class_priors = class_counts / len(Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e6b969",
      "metadata": {
        "scrolled": true,
        "id": "10e6b969"
      },
      "outputs": [],
      "source": [
        "#convert text document from X_train abstract column to matrix of counts by using vectorizer\n",
        "#specify to not include in common english words or words that occur over 90% in the documents\n",
        "#include counts of single worlds, and concatonated words up to 3 words next to each other in the matrix\n",
        "vectorizer = CountVectorizer(binary=True,stop_words='english', ngram_range=(1,3),max_df=0.9)\n",
        "X_train = vectorizer.fit_transform(trg['abstract'])\n",
        "\n",
        "#assign class label column as Y_train and test data as X_test to be used when predicting\n",
        "Y_train = trg['Category']\n",
        "X_test = vectorizer.transform(tst['abstract'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write out test and training datasets to be used for model implementation"
      ],
      "metadata": {
        "id": "bZgxBevK_Xuh"
      },
      "id": "bZgxBevK_Xuh"
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dataframe pased on the predicted labels of naive bayes classifier\n",
        "df = pd.DataFrame(X_test)\n",
        "# Reset the index to start from 1\n",
        "df.index += 1\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df.to_csv('Articles test dataset.csv',index= True, index_label = 'Text')\n",
        "\n",
        "df = pd.DataFrame(Y_train)\n",
        "# Reset the index to start from 1\n",
        "df.index += 1\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df.to_csv('Articles train dataset.csv',index= True, index_label = 'Category')"
      ],
      "metadata": {
        "id": "5A7YQ8rW_XHL"
      },
      "id": "5A7YQ8rW_XHL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}